---
title: "Course Project Report"
author: "Jenny Wan"
date: "September 17, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

In this project, a machine learning model is fitted to predict the manner in which people did their exercise using the data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. The caret package in R is used to implement the machine learning algorithm. The accuracy is estimated using 5-fold cross validation. Finally, 20 different cases are predicted using the model built previously.

```{r}
library(caret)
```


## Selecting Predictors

The predictors are selected by observing the content of raw data file. The raw training data downloaded from course website contains 19622 observations and 159 variables. It's too time consuming and does not make sense to predict the model blindly with all the other variables. After taking a look at the data, it turns out that some columns are partially empty/filled with N/A values, so obviously they should be eliminated first.  
Variables such as the order of data (1st column), user name, time stamps and number of windows are not relevant to the prediction. So they are removed from the data sheet manually. The variables that are closely related to the accelerometers are selected. The variables that start with "total_accel_" also do not show much variance and do not seem to be relevant to the classe. They are also removed from the predictor list. 
After making the selection, only 48 predictors are left and will be used in building the prediction model. The final variables selected are listed below:

```{r, echo = FALSE}
training = read.csv("~/Dropbox/Practical Machine Learning/Project/pml-training-copy.csv")
names(training)
```

# Model and Cross Validation

The problem proposed is a classification problem. There are still 48 predictors exist in the revised data file. Random forest method is thus selected to build vairous classification trees and vote for the final prediction. 
To estimate the out of sample accuracy, a 5-fold cross validation is used when training the model. Below are the codes for setting up cross validation and building the model:
```{r}
train_control <- trainControl(method = "cv", number = 5)
modFit <- train(classe~., data = training, trControl = train_control, method = "rf" )
print(modFit)
```

Therefore, the out of sample error is estimated to be (1-0.9945978) = 0.0054022. This number sounds really small, indicating the model proposed is very precise in predicting. But this number is still overestimated as k equals only 5, so the variance is expected to be smaller and the bias is expected to be larger. 

# Discussion

The model is then applied to the testing data set:

```{r}
testing = read.csv("~/Dropbox/Practical Machine Learning/Project/pml-testing-copy.csv")
outcome <- predict(modFit, newdata = testing)
```


After applying the model built previously to the testing set (with all the redundant variables removed already), the outcome is shown below:
```{r}
outcome
```

They are all correct predictions. This result is not surprising as the accuracy is very high.

However, building the model does take a considerablely long time (~1hr running in R). Increasing the k value in k-fold cross validation could further increase the running time. 
